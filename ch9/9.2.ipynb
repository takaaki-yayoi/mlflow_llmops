{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エージェント型RAGシステムの構築 - MLflow & LangGraph Tutorial\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、LangGraphとMLflowを組み合わせて、自律的に判断し行動するエージェント型RAG（Retrieval-Augmented Generation）システムを構築する方法を学びます。\n",
    "\n",
    "### 学習内容\n",
    "1. ベクトルデータベース（Chroma）の構築と文書の格納\n",
    "2. LangGraphによるマルチノードワークフローの設計\n",
    "3. エージェント型RAGの実装（質問のルーティング、検索、再試行ロジック）\n",
    "4. MLflow Tracingによる処理の可観測性向上\n",
    "5. MLflow Evaluationによる自動評価\n",
    "6. モデルの記録と本番環境へのデプロイ準備\n",
    "\n",
    "### エージェント型RAGとは？\n",
    "従来のRAGは「必ず検索→回答」という固定フローでしたが、エージェント型RAGは：\n",
    "- 質問内容に応じて検索の要否を自動判断\n",
    "- 検索結果の品質を評価し、不十分な場合は質問を改善して再検索\n",
    "- 複数の処理経路を動的に選択\n",
    "\n",
    "これにより、より柔軟で精度の高い回答生成が可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ1: 環境セットアップ\n",
    "\n",
    "### 必要なライブラリ\n",
    "- `mlflow`: 実験管理、モデル記録、トレーシング\n",
    "- `langchain[openai]`: LangChainとOpenAI連携\n",
    "- `langgraph`: ワークフローグラフの構築（状態管理、条件分岐）\n",
    "- `chromadb`: ベクトルデータベース\n",
    "- `langchain-community`: コミュニティ拡張（ベクトルストア等）\n",
    "- `langchain-text-splitters`: テキスト分割ツール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"mlflow\" \"langchain[openai]\" langgraph chromadb langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ2: 認証情報の設定\n",
    "\n",
    "OpenAI APIを使用するため、APIキーを環境変数に設定します。\n",
    "\n",
    "**重要**: \n",
    "- `YOUR_API_KEY`を実際のAPIキーに置き換えてください\n",
    "- 本番環境では、シークレット管理サービスを使用することを推奨します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ3: ベクトルデータベースの構築\n",
    "\n",
    "### RAGにおけるベクトルデータベースの役割\n",
    "テキストを数値ベクトル（埋め込み）に変換して保存し、意味的に類似した文書を高速に検索できるようにします。\n",
    "\n",
    "### このステップで行うこと\n",
    "1. **サンプル文書の準備**: 社内技術文書を想定したテキストデータ\n",
    "2. **テキスト分割（チャンキング）**: 長い文書を適切なサイズに分割\n",
    "3. **埋め込み生成**: OpenAIの埋め込みモデルでベクトル化\n",
    "4. **Chromaへの保存**: ベクトルデータベースに永続化\n",
    "\n",
    "### チャンキングのパラメータ\n",
    "- `chunk_size=300`: 各チャンクの最大文字数\n",
    "- `chunk_overlap=50`: チャンク間のオーバーラップ（文脈の連続性を保つため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAGで使うベクトルデータベース（Chroma）を準備するスクリプトです。\n",
    "・社内技術文書などを想定したサンプルテキストをいくつか登録します。\n",
    "・LangChainのTextSplitterでチャンクに分割し、Chromaに保存します。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def build_vectorstore(persist_dir: str = \"./chroma_store\") -> Chroma:\n",
    "    \"\"\"\n",
    "    ベクトルデータベースを構築する関数\n",
    "\n",
    "    Args:\n",
    "        persist_dir: ベクトルデータベースの保存先ディレクトリ\n",
    "\n",
    "    Returns:\n",
    "        構築されたChromaベクトルストア\n",
    "    \"\"\"\n",
    "    # サンプルの社内技術文書（本番ではファイルやデータベースから読み込む）\n",
    "    raw_docs = [\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"MLflowは機械学習および生成AIの実験管理、モデル管理、\"\n",
    "                \"そして可観測性を提供するオープンソースのプラットフォームです。\"\n",
    "            ),\n",
    "            metadata={\"source\": \"mlflow_intro\"},\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"RAG（Retrieval-Augmented Generation）は、外部のナレッジベースから\"\n",
    "                \"関連文書を検索し、その内容をもとに回答を生成する手法です。\"\n",
    "            ),\n",
    "            metadata={\"source\": \"rag_intro\"},\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Chromaはシンプルに使えるベクトルデータベースであり、\"\n",
    "                \"Pythonからの利用に適しています。我が社ではRAG用のデフォルトDBとして使用されています。\"\n",
    "            ),\n",
    "            metadata={\"source\": \"chroma_intro\"},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # テキストをチャンクに分割\n",
    "    # RecursiveCharacterTextSplitter: 段落、文、単語の順で自然な区切りを探す\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,      # 各チャンクの最大文字数\n",
    "        chunk_overlap=50,    # チャンク間のオーバーラップ（文脈保持のため）\n",
    "    )\n",
    "    splits = splitter.split_documents(raw_docs)\n",
    "\n",
    "    # OpenAIの埋め込みモデルを使ってChromaを構築\n",
    "    # 埋め込み: テキストを高次元ベクトルに変換（意味的類似性の計算に使用）\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "    )\n",
    "\n",
    "    # ディスクに永続化（プログラム終了後も利用可能）\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "# ベクトルストアを構築\n",
    "build_vectorstore()\n",
    "print(\"Chromaストアを作成しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ4: エージェント型RAGアプリケーションの実装\n",
    "\n",
    "### LangGraphとは？\n",
    "状態を持つマルチエージェントワークフローを構築するためのフレームワークです。\n",
    "複雑な処理フローを「ノード（処理単位）」と「エッジ（遷移）」のグラフとして表現します。\n",
    "\n",
    "### このアプリケーションの構成\n",
    "\n",
    "#### 5つのノード\n",
    "1. **router**: 質問を分析し、検索の要否を判定\n",
    "2. **retrieve**: ベクトルDBから関連文書を検索\n",
    "3. **check**: 検索結果の品質を評価\n",
    "4. **rewrite**: 質問を検索に適した形に改善\n",
    "5. **answer**: 最終回答を生成\n",
    "\n",
    "#### 処理フロー\n",
    "```\n",
    "START → router → [検索必要？]\n",
    "             ├─ YES → retrieve → check → [品質十分？]\n",
    "             │                        ├─ YES → answer → END\n",
    "             │                        └─ NO → rewrite → router（再試行）\n",
    "             └─ NO → answer → END（LLMの知識のみで回答）\n",
    "```\n",
    "\n",
    "### MLflow Tracingの活用\n",
    "各ノードの処理内容、入出力、実行時間をトレースとして記録し、デバッグや性能分析を容易にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./agentic_rag_app.py\n",
    "\"\"\"\n",
    "最小構成のエージェント型RAGアプリケーションの例です。\n",
    "\n",
    "・LangGraphで次の5ノードを持つグラフを作成します:\n",
    "  - router: 質問を見て「検索すべきかどうか」を決める\n",
    "  - retrieve: Chromaから関連文書を検索する\n",
    "  - check: 検索結果が十分かどうかを判定する\n",
    "  - rewrite: 質問を少し言い換える\n",
    "  - answer: コンテキスト＋質問から最終回答を生成する\n",
    "\n",
    "・MLflow Tracingで、各ノードの処理をスパンとして記録します。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Literal, Dict, Any\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities import SpanType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# ==========\n",
    "# 事前準備\n",
    "# ==========\n",
    "\n",
    "# LangChainのLLM（温度0で決定的な出力）\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# Chromaベクトルストア（前のステップで作成したものを読み込み）\n",
    "PERSIST_DIR = \"./chroma_store\"\n",
    "vectordb = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=PERSIST_DIR,\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# LangChain/MLflowの自動ロギングを有効化\n",
    "# LLM呼び出しやチェーン実行を自動的にMLflowに記録\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "# ==========\n",
    "# 検索関数\n",
    "# ==========\n",
    "\n",
    "def retrieve_docs(query: str) -> str:\n",
    "    \"\"\"\n",
    "    質問文から関連文書を検索し、テキストを1つの文字列として返します。\n",
    "    \n",
    "    Args:\n",
    "        query: 検索クエリ（ユーザーの質問）\n",
    "    \n",
    "    Returns:\n",
    "        検索結果の文書を改行で連結した文字列\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# ==========\n",
    "# カスタム状態の定義\n",
    "# ==========\n",
    "\n",
    "class AgenticRAGState(TypedDict):\n",
    "    \"\"\"\n",
    "    LangGraphのワークフロー全体で共有される状態\n",
    "    \n",
    "    Attributes:\n",
    "        messages: 会話履歴（質問、中間メッセージ、回答など）\n",
    "        route: routerノードの判定結果（'rag' or 'llm_only'）\n",
    "        context: 検索結果のテキスト\n",
    "        check_result: checkノードの判定結果（'answer' or 'rewrite'）\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]  # 自動的にメッセージを追加\n",
    "    route: str\n",
    "    context: str\n",
    "    check_result: str\n",
    "\n",
    "\n",
    "# ==========\n",
    "# ノード定義\n",
    "# ==========\n",
    "\n",
    "def router_node(state: AgenticRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    【ノード1: ルーター】\n",
    "    質問を見て「検索すべきかどうか」を判定するノードです。\n",
    "    \n",
    "    判定ロジック:\n",
    "    - 社内文書に関する質問 → 'rag'（検索が必要）\n",
    "    - 一般常識や外部知識の質問 → 'llm_only'（LLMの知識のみで回答）\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = (\n",
    "        \"次の質問に答えるために、社内の技術文書を検索した方がよいかを判定してください。\\n\"\n",
    "        \"検索した方がよい場合は 'rag'、不要な場合は 'llm_only' とだけ答えてください。\\n\\n\"\n",
    "        f\"質問: {question}\"\n",
    "    )\n",
    "    res = llm.invoke([HumanMessage(content=prompt)])\n",
    "    decision = res.content.strip().lower()\n",
    "\n",
    "    # トレースにルート情報をタグとして残す（後で分析しやすくするため）\n",
    "    mlflow.update_current_trace(tags={\"route_decision\": decision})\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=f\"[route={decision}]\")], \"route\": decision}\n",
    "\n",
    "\n",
    "def retrieve_node(state: AgenticRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    【ノード2: 検索】\n",
    "    Chromaベクトルデータベースから関連文書を検索するノードです。\n",
    "    \n",
    "    処理内容:\n",
    "    1. ユーザーの質問（最初のメッセージ）を取得\n",
    "    2. ベクトル検索を実行\n",
    "    3. 検索結果をstateのcontextに格納\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = retrieve_docs(question)\n",
    "\n",
    "    # 検索結果をstateに追加\n",
    "    return {\"messages\": [AIMessage(content=context)], \"context\": context}\n",
    "\n",
    "\n",
    "def check_node(state: AgenticRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    【ノード3: 品質チェック】\n",
    "    検索結果のテキスト（context）が質問に十分関連しているかどうかを判定します。\n",
    "    \n",
    "    判定基準:\n",
    "    - 関連性が高い → 'answer'（そのまま回答生成へ）\n",
    "    - 関連性が低い → 'rewrite'（質問を改善して再検索）\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    prompt = (\n",
    "        \"あなたは、検索で得られた文書が質問に関連しているかどうかを判定する役割です。\\n\"\n",
    "        \"関連していれば 'yes'、ほとんど関係なければ 'no' とだけ回答してください。\\n\\n\"\n",
    "        f\"質問: {question}\\n\\n\"\n",
    "        f\"検索結果: {context[:2000]}\\n\"  # 長すぎる場合は最初の2000文字のみ使用\n",
    "    )\n",
    "    res = llm.invoke([HumanMessage(content=prompt)])\n",
    "    score = res.content.strip().lower()\n",
    "\n",
    "    decision = \"answer\" if score == \"yes\" else \"rewrite\"\n",
    "\n",
    "    # トレースに判定結果をタグとして保存\n",
    "    mlflow.update_current_trace(tags={\"check_decision\": decision})\n",
    "\n",
    "    # 判定結果をstateに保存\n",
    "    return {\"check_result\": decision}\n",
    "\n",
    "\n",
    "def rewrite_node(state: AgenticRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    【ノード4: 質問の改善】\n",
    "    質問を検索に適した形に言い換えるノードです。\n",
    "    \n",
    "    目的:\n",
    "    - 曖昧な表現を具体化\n",
    "    - 検索に適したキーワードを含める\n",
    "    - 元の意図は保持\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    prompt = (\n",
    "        \"次の質問文を、検索に適した形になるように言い換えてください。\\n\"\n",
    "        \"ただし、意味や意図は変えないでください。\\n\\n\"\n",
    "        f\"元の質問: {question}\"\n",
    "    )\n",
    "    res = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # 新しい質問をMessagesStateに追加して、再度routerに戻します\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n",
    "\n",
    "\n",
    "def answer_node(state: AgenticRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    【ノード5: 回答生成】\n",
    "    検索結果のコンテキストと質問を使って最終回答を生成するノードです。\n",
    "    contextがない場合は、LLMの知識のみで回答します。\n",
    "    \n",
    "    2つのモード:\n",
    "    1. RAGモード: 検索結果に基づいて回答（ハルシネーション防止）\n",
    "    2. LLMモード: LLMの内部知識のみで回答\n",
    "    \"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    if context:\n",
    "        # RAGルート: コンテキストに基づいて回答\n",
    "        prompt = (\n",
    "            \"以下のコンテキストに基づいて質問に答えてください。\\n\"\n",
    "            \"コンテキストに書かれていないことは推測せず、「わかりません」と答えてください。\\n\"\n",
    "            \"3文以内で、簡潔でわかりやすい日本語で答えてください。\\n\\n\"\n",
    "            f\"質問: {question}\\n\\n\"\n",
    "            f\"コンテキスト:\\n{context}\"\n",
    "        )\n",
    "    else:\n",
    "        # LLM単体ルート: LLMの知識で直接回答\n",
    "        prompt = (\n",
    "            \"以下の質問に、あなたの知識に基づいて答えてください。\\n\"\n",
    "            \"3文以内で、簡潔でわかりやすい日本語で答えてください。\\n\\n\"\n",
    "            f\"質問: {question}\"\n",
    "        )\n",
    "\n",
    "    res = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"messages\": [res]}\n",
    "\n",
    "\n",
    "# ==========\n",
    "# グラフ定義\n",
    "# ==========\n",
    "\n",
    "def build_agentic_rag_graph():\n",
    "    \"\"\"\n",
    "    LangGraphでエージェント型RAGのワークフローグラフを組み立てます。\n",
    "    \n",
    "    グラフ構造:\n",
    "    - ノード: 処理単位（router, retrieve, check, rewrite, answer）\n",
    "    - エッジ: ノード間の遷移（固定エッジと条件付きエッジ）\n",
    "    - 条件分岐: routerとcheckの判定結果に応じて次のノードを動的に決定\n",
    "    \"\"\"\n",
    "    workflow = StateGraph(AgenticRAGState)\n",
    "\n",
    "    # 5つのノードを登録\n",
    "    workflow.add_node(\"router\", router_node)\n",
    "    workflow.add_node(\"retrieve\", retrieve_node)\n",
    "    workflow.add_node(\"check\", check_node)\n",
    "    workflow.add_node(\"rewrite\", rewrite_node)\n",
    "    workflow.add_node(\"answer\", answer_node)\n",
    "\n",
    "    # 開始点: まずrouterノードから開始\n",
    "    workflow.add_edge(START, \"router\")\n",
    "\n",
    "    # routerの判定結果に応じて分岐\n",
    "    def route_decision(state: AgenticRAGState) -> Literal[\"retrieve\", \"answer\"]:\n",
    "        \"\"\"routerの判定結果に基づいて次のノードを決める\"\"\"\n",
    "        route = state.get(\"route\", \"rag\")\n",
    "        return \"retrieve\" if route == \"rag\" else \"answer\"\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_decision,\n",
    "        {\n",
    "            \"retrieve\": \"retrieve\",  # 検索が必要 → retrieveノードへ\n",
    "            \"answer\": \"answer\",      # 検索不要 → answerノードへ\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # retrieve後は必ずcheckに進む（固定エッジ）\n",
    "    workflow.add_edge(\"retrieve\", \"check\")\n",
    "\n",
    "    # checkの判定結果に応じて分岐\n",
    "    def check_decision(state: AgenticRAGState) -> Literal[\"answer\", \"rewrite\"]:\n",
    "        \"\"\"checkの判定結果に基づいて次のノードを決める\"\"\"\n",
    "        return state.get(\"check_result\", \"answer\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"check\",\n",
    "        check_decision,\n",
    "        {\n",
    "            \"answer\": \"answer\",    # 品質OK → 回答生成へ\n",
    "            \"rewrite\": \"rewrite\",  # 品質不足 → 質問改善へ\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # rewrite後は再度routerへ戻る（再試行ループ）\n",
    "    workflow.add_edge(\"rewrite\", \"router\")\n",
    "\n",
    "    # answer後は終了\n",
    "    workflow.add_edge(\"answer\", END)\n",
    "\n",
    "    # グラフをコンパイルして実行可能な状態にする\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# グラフを構築\n",
    "graph = build_agentic_rag_graph()\n",
    "\n",
    "# MLflowのModels from Codeパターンで登録できるようにする\n",
    "mlflow.models.set_model(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ5: エージェント型RAGの実行とテスト\n",
    "\n",
    "構築したエージェントに質問を投げて、実際の動作を確認します。\n",
    "\n",
    "### 実行フロー\n",
    "1. 質問をメッセージ形式で準備\n",
    "2. `graph.invoke()`で実行\n",
    "3. 最終状態から回答を取得\n",
    "4. MLflow Tracingで処理の詳細を記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# エージェント型RAGの実行\n",
    "# ==========\n",
    "import mlflow\n",
    "from agentic_rag_app import graph\n",
    "\n",
    "# テスト用の質問\n",
    "question = \"Chromaの弊社での位置付けを教えて\"\n",
    "\n",
    "# トレースに質問内容をタグとして保存（後から分析しやすくするため）\n",
    "mlflow.update_current_trace(tags={\"question\": question})\n",
    "\n",
    "# LangGraphに渡す初期State（messagesにユーザー質問を入れる）\n",
    "input_example = {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    "\n",
    "# graph.invokeで実行し、最終状態を取得\n",
    "# 内部で router → retrieve → check → answer と処理が進む\n",
    "final_state = graph.invoke(input_example)\n",
    "\n",
    "# 最後のメッセージを最終回答とみなす\n",
    "last_msg = final_state[\"messages\"][-1]\n",
    "print(\"質問:\", question)\n",
    "print(\"回答:\", last_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ6: ワークフローの可視化\n",
    "\n",
    "LangGraphの強力な機能の1つが、グラフ構造の可視化です。\n",
    "ノード間の関係、条件分岐、ループ構造を視覚的に確認できます。\n",
    "\n",
    "### 可視化のメリット\n",
    "- ワークフローの全体像を把握\n",
    "- デバッグ時に処理フローを追跡\n",
    "- チームメンバーとの共有・ドキュメント化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # エージェントのグラフ構造を可視化（Mermaid形式で描画）\n",
    "    graph_image = graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"✓ ワークフローの図を表示しました\")\n",
    "except Exception as e:\n",
    "    print(f\"図の表示に失敗しました: {e}\")\n",
    "    print(\"（この機能は環境によっては動作しない場合があります）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ7: MLflowへのモデル記録\n",
    "\n",
    "### LangChainフレーバー\n",
    "MLflowは、LangChainで構築したアプリケーションを専用のフレーバーとして記録できます。\n",
    "これにより、依存関係、入力スキーマ、実行環境がすべて保存されます。\n",
    "\n",
    "### 記録内容\n",
    "- グラフの定義（agentic_rag_app.py）\n",
    "- 依存パッケージ（自動推論）\n",
    "- 入力例（スキーマ推論用）\n",
    "- 実行環境の情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflowの設定（ローカルTrackingサーバーを想定）\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"agentic_rag_example\")\n",
    "\n",
    "# LangGraphで構築したグラフをMLflowに記録\n",
    "with mlflow.start_run(run_name=\"my-agentic-rag\") as run:\n",
    "    # LangChainフレーバーとして記録\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        lc_model=\"./agentic_rag_app.py\",  # LangGraphアプリのコード\n",
    "        artifact_path=\"model\",             # モデルの保存先\n",
    "        input_example=input_example,       # 入力スキーマ推論用\n",
    "    )\n",
    "\n",
    "print(\"モデルURI:\", model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ8: モデルのロードと推論テスト\n",
    "\n",
    "記録したモデルを読み込み、推論が正しく動作することを確認します。\n",
    "\n",
    "### PyFuncとして読み込むメリット\n",
    "- 統一されたインターフェース（predict()メソッド）\n",
    "- フレームワーク非依存のデプロイが可能\n",
    "- REST APIとして簡単にサービング可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記録したモデルを読み込む\n",
    "loaded = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# テスト推論を実行\n",
    "result = loaded.predict(input_example)\n",
    "print(\"推論結果:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ9: MLflow Evaluationによる自動評価\n",
    "\n",
    "### MLflow Evaluationとは？\n",
    "モデルの品質を自動的に評価する機能です。LLM-as-a-Judgeパターンを使用し、\n",
    "別のLLMが回答の品質を採点します。\n",
    "\n",
    "### 評価指標\n",
    "- **Correctness**: 回答の正しさ（期待回答との一致度）\n",
    "- **RetrievalSufficiency**: 検索結果の十分性（質問に答えるのに十分な情報があるか）\n",
    "\n",
    "### 評価データセット\n",
    "質問と期待回答のペアを用意し、モデルの出力と比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "エージェント型RAGアプリケーションをMLflow Evaluationで評価する例です。\n",
    "・小さな評価データセットを用意し、\n",
    "・エージェント型RAG関数をpredict_fnとして渡し、\n",
    "・LLM-as-a-Judgeのスコア（正しさ・関連性）を計算します。\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness, RetrievalSufficiency\n",
    "\n",
    "# 1. 評価データセットを用意\n",
    "# 各エントリは入力（質問）と期待される出力のペア\n",
    "dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"RAGとは何か、簡単に説明してください。\",\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"外部の文書を検索して、その内容に基づいて回答を生成する仕組みであること\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"Chromaの弊社での位置付けを教えて\",\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"Chromaは、弊社ではRAG用のデフォルトDBとして位置付けられています\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# 2. エージェント型RAGを呼び出すpredict_wrapperを定義\n",
    "def predict_wrapper(question: str) -> str:\n",
    "    \"\"\"\n",
    "    MLflow Evaluationから呼び出される予測関数です。\n",
    "\n",
    "    Args:\n",
    "        question: ユーザーの質問文\n",
    "\n",
    "    Returns:\n",
    "        エージェント型RAGからの回答テキスト\n",
    "    \"\"\"\n",
    "    # トレースに質問内容をタグとして保存\n",
    "    mlflow.update_current_trace(tags={\"question\": question})\n",
    "\n",
    "    # LangGraphに渡す初期State\n",
    "    input_example = {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    "\n",
    "    # graph.invokeで実行し、最終状態を取得\n",
    "    final_state = loaded.predict(input_example)\n",
    "\n",
    "    # 最後のメッセージを最終回答とみなす\n",
    "    last_msg = final_state[0][\"messages\"][-1]\n",
    "    return last_msg\n",
    "\n",
    "# 3. Evaluationの実行\n",
    "# 評価結果はMLflowに自動的に記録されます\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.genai.evaluate(\n",
    "        data=dataset,                  # 評価データセット\n",
    "        predict_fn=predict_wrapper,    # 予測関数\n",
    "        scorers=[                      # 評価指標のリスト\n",
    "            Correctness(),             # 正しさの評価\n",
    "            RetrievalSufficiency(),    # 検索の十分性評価\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(\"評価が完了しました。MLflow UIで結果を確認してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ10: モデルレジストリへの登録\n",
    "\n",
    "### モデルレジストリとは？\n",
    "本番環境へのデプロイ準備として、モデルに名前を付けて一元管理する仕組みです。\n",
    "\n",
    "### メリット\n",
    "- バージョン管理: 複数バージョンの並行管理\n",
    "- エイリアス設定: champion, staging等のラベル付け\n",
    "- デプロイ追跡: どのバージョンが本番稼働中かを記録\n",
    "- チーム共有: モデルの中央リポジトリとして機能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをモデルレジストリに登録\n",
    "# 登録すると、バージョン番号が自動的に付与されます\n",
    "mlflow.register_model(\n",
    "    model_uri=model_info.model_uri, \n",
    "    name=\"agentic-rag-model\"\n",
    ")\n",
    "\n",
    "print(\"モデルを 'agentic-rag-model' として登録しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **ベクトルデータベース構築**: Chromaを使った文書の埋め込みと保存\n",
    "2. **LangGraphによるワークフロー設計**: 複雑な処理フローをグラフで表現\n",
    "3. **エージェント型RAG**: 動的な判断と再試行を含む高度なRAGシステム\n",
    "4. **MLflow Tracing**: 処理の可観測性向上とデバッグ支援\n",
    "5. **MLflow Evaluation**: LLM-as-a-Judgeによる自動評価\n",
    "6. **モデル管理**: LangChainモデルの記録、バージョン管理、デプロイ準備\n",
    "\n",
    "### エージェント型RAGの利点\n",
    "\n",
    "- **適応的な検索**: 質問に応じて検索の要否を自動判断\n",
    "- **品質管理**: 検索結果の妥当性を評価し、必要に応じて再試行\n",
    "- **柔軟性**: 複数の処理経路を持ち、状況に応じた最適な回答生成\n",
    "- **可観測性**: MLflow Tracingで全処理を追跡可能\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- より大規模な文書コレクションでの実験\n",
    "- 他のベクトルデータベース（Pinecone, Weaviate等）の試用\n",
    "- カスタム評価指標の追加\n",
    "- 本番環境へのデプロイ（REST API、バッチ処理等）\n",
    "- A/Bテストによる異なるプロンプト戦略の比較\n",
    "- エラーハンドリングとフォールバック機能の強化\n",
    "\n",
    "### 参考リソース\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)\n",
    "- [MLflow Evaluation for LLMs](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [Chroma Vector Database](https://docs.trychroma.com/)"
   ]
  }
 ]
}