{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書情報抽出モデルの構築 - MLflow Tutorial\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、MLflowを使用して日本語のビジネス文書から情報を抽出するAIモデルを構築し、管理する方法を学びます。\n",
    "\n",
    "### 学習内容\n",
    "1. MLflow Prompt Registryの使い方\n",
    "2. Models from Codeパターンの実装\n",
    "3. カスタムPyFuncモデルの作成\n",
    "4. モデルのバージョン管理とエイリアス設定\n",
    "5. モデルのサービング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ1: 環境セットアップ\n",
    "\n",
    "### 必要なライブラリのインストール\n",
    "- `mlflow[genai]`: MLflowの生成AI機能（Prompt Registryなど）\n",
    "- `openai`: OpenAI APIクライアント\n",
    "- `pandas`: データフレーム操作用\n",
    "\n",
    "**注意**: インストール後、Pythonランタイムを再起動して変更を反映させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U mlflow[genai] openai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ2: 認証情報の設定\n",
    "\n",
    "OpenAI APIを使用するため、APIキーを環境変数に設定します。\n",
    "\n",
    "**重要**: \n",
    "- `YOUR_API_KEY`を実際のAPIキーに置き換えてください\n",
    "- 本番環境では、Databricks Secretsを使用することを推奨します\n",
    "  ```python\n",
    "  os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"my-scope\", key=\"openai-api-key\")\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ3: MLflow実験の設定とプロンプト登録\n",
    "\n",
    "### MLflow Trackingとは？\n",
    "機械学習実験の記録・管理システムです。パラメータ、メトリクス、モデルを一元管理できます。\n",
    "\n",
    "### Prompt Registryとは？\n",
    "プロンプトをバージョン管理し、複数のモデルで再利用できる仕組みです。\n",
    "プロンプトを変更した際の影響追跡や、A/Bテストが容易になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflow Trackingの出力先を設定（ローカルサーバーの例）\n",
    "# Databricks環境では自動的にワークスペースのMLflowを使用します\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# 実験名を設定 - 関連する実行をグループ化します\n",
    "mlflow.set_experiment(\"document_extraction_minimal\")\n",
    "\n",
    "# プロンプトの内容を文字列として用意\n",
    "# このプロンプトは、LLMに対して構造化されたJSON形式で情報を抽出するよう指示します\n",
    "template_text = \"\"\"\n",
    "あなたは日本語のビジネス文書から情報を抽出する補助者です。\n",
    "入力として任意のテキストを与えます。\n",
    "以下のJSON形式に従って、必ず有効なJSONだけを返してください。\n",
    "\n",
    "- company_name: 会社名（不明な場合はnull）\n",
    "- contract_start_date: 契約開始日（YYYY-MM-DD形式。不明な場合はnull）\n",
    "- contract_end_date: 契約終了日（YYYY-MM-DD形式。不明な場合はnull）\n",
    "- monthly_fee_jpy: 月額料金（数値。不明な場合はnull）\n",
    "- plan_name: プラン名（不明な場合はnull）\n",
    "\n",
    "制約:\n",
    "- 回答はJSONオブジェクト1つだけを返してください。\n",
    "- 余計な文章やコメント、日本語の説明は一切書かないでください。\n",
    "- nullを使う場合は、小文字のnullを使ってください。\n",
    "\n",
    "入力テキスト:\n",
    "{{text}}\n",
    "\n",
    "出力形式の例:\n",
    "{\n",
    "  \"company_name\": \"株式会社サンプル\",\n",
    "  \"contract_start_date\": \"2025-01-01\",\n",
    "  \"contract_end_date\": \"2025-12-31\",\n",
    "  \"monthly_fee_jpy\": 120000,\n",
    "  \"plan_name\": \"プレミアム\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# プロンプトをMLflow Prompt Registryに登録\n",
    "# これにより、プロンプトのバージョン管理と再利用が可能になります\n",
    "mlflow.genai.register_prompt(\n",
    "    name=\"document-extraction-system\",\n",
    "    template=template_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ4: カスタムモデルクラスの定義\n",
    "\n",
    "### Models from Codeパターン\n",
    "コードを直接MLflowモデルとして登録する手法です。以下のメリットがあります：\n",
    "- モデルとコードが一体化し、再現性が向上\n",
    "- 依存関係が明示的に管理される\n",
    "- デプロイが容易\n",
    "\n",
    "### このセルの動作\n",
    "`%%writefile`マジックコマンドで、セルの内容を外部ファイルとして保存します。\n",
    "\n",
    "### 主要コンポーネントの説明\n",
    "\n",
    "#### 1. DocumentExtractionModelクラス\n",
    "- MLflowの`PythonModel`を継承したカスタムモデル\n",
    "- `load_context()`: モデルロード時の初期化処理\n",
    "- `predict()`: 推論処理のメインロジック\n",
    "\n",
    "#### 2. トレーシング機能\n",
    "- `@mlflow.trace()`: 各関数の実行を記録し、デバッグや性能分析に活用\n",
    "- `SpanType.TOOL`, `SpanType.LLM`, `SpanType.CHAIN`で処理の種類を分類\n",
    "\n",
    "#### 3. エラー対策\n",
    "- OpenAIのJSON mode使用時、systemメッセージに\"json\"を含めることでBAD_REQUESTエラーを回避"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./document_extraction_model.py\n",
    "import json\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models import set_model\n",
    "from mlflow.entities import SpanType\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# デフォルト設定\n",
    "DEFAULT_PROMPT_URI = \"prompts:/document-extraction-system/1\"\n",
    "DEFAULT_LLM_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# OpenAI APIの呼び出しを自動的にMLflowに記録\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "def _load_prompt(prompt_uri: str):\n",
    "    \"\"\"\n",
    "    Prompt Registryからプロンプトをロードする関数\n",
    "    URI形式: prompts://<プロンプト名>/<バージョン>\n",
    "    \"\"\"\n",
    "    return mlflow.genai.load_prompt(prompt_uri)\n",
    "\n",
    "@mlflow.trace(span_type=SpanType.TOOL)\n",
    "def _render_prompt(prompt, text: str) -> str:\n",
    "    \"\"\"\n",
    "    プロンプトテンプレートに実際のテキストを埋め込む関数\n",
    "    {{text}}プレースホルダーを入力テキストで置換します\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return prompt.format(text=text)\n",
    "    except Exception:\n",
    "        # フォールバック: format()が使えない場合は文字列置換\n",
    "        tmpl = getattr(prompt, \"template\", None)\n",
    "        if isinstance(tmpl, str):\n",
    "            return tmpl.replace(\"{{text}}\", text)\n",
    "        return str(prompt).replace(\"{{text}}\", text)\n",
    "\n",
    "@mlflow.trace(span_type=SpanType.LLM)\n",
    "def _call_llm_return_json(*, client, prompt_text: str, model: str, max_tokens: int, temperature: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    OpenAI APIを呼び出し、JSON形式で結果を返す関数\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAIクライアント\n",
    "        prompt_text: 生成済みのプロンプト全文\n",
    "        model: 使用するLLMモデル名\n",
    "        max_tokens: 最大トークン数\n",
    "        temperature: 生成のランダム性（0=決定的、1=創造的）\n",
    "    \n",
    "    Returns:\n",
    "        抽出された情報を含む辞書\n",
    "    \"\"\"\n",
    "    res = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Return ONLY valid JSON (json_object).\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_text},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        response_format={\"type\": \"json_object\"},  # JSON形式を強制\n",
    "    )\n",
    "    content = res.choices[0].message.content\n",
    "    return json.loads(content)\n",
    "\n",
    "\n",
    "class DocumentExtractionModel(PythonModel):\n",
    "    \"\"\"\n",
    "    ビジネス文書から情報を抽出するカスタムMLflowモデル\n",
    "    \n",
    "    入力形式: pandas.DataFrame\n",
    "        - 必須カラム: 'text' (抽出対象のテキスト)\n",
    "        - オプションカラム: 'model' (使用するLLMモデル名)\n",
    "    \n",
    "    出力形式: pandas.DataFrame\n",
    "        - 抽出されたJSON項目が各カラムとして返される\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        モデルロード時に1回だけ実行される初期化メソッド\n",
    "        設定の読み込み、クライアントの初期化、プロンプトのロードを行います\n",
    "        \"\"\"\n",
    "        # model_configから設定を取得\n",
    "        self.cfg = getattr(context, \"model_config\", {}) or {}\n",
    "        \n",
    "        # OpenAIクライアントを初期化（環境変数からAPIキーを取得）\n",
    "        self.client = OpenAI()\n",
    "        \n",
    "        # 設定値を取得（デフォルト値を指定）\n",
    "        self.prompt_uri = self.cfg.get(\"prompt_uri\", DEFAULT_PROMPT_URI)\n",
    "        self.default_model = self.cfg.get(\"default_model\", DEFAULT_LLM_MODEL)\n",
    "        self.max_tokens = int(self.cfg.get(\"max_tokens\", 1024))\n",
    "        self.temperature = float(self.cfg.get(\"temperature\", 0.0))\n",
    "        \n",
    "        # Prompt Registryからプロンプトをロード\n",
    "        self.prompt = _load_prompt(self.prompt_uri)\n",
    "    \n",
    "    @mlflow.trace(span_type=SpanType.CHAIN)\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        推論メソッド - 入力テキストから情報を抽出します\n",
    "        \n",
    "        処理フロー:\n",
    "        1. 入力の検証とDataFrame化\n",
    "        2. 各行に対してループ処理\n",
    "        3. プロンプトのレンダリング\n",
    "        4. LLM呼び出し\n",
    "        5. 結果の収集とDataFrame化\n",
    "        \"\"\"\n",
    "        # 入力がDataFrameでない場合は変換\n",
    "        if not isinstance(model_input, pd.DataFrame):\n",
    "            model_input = pd.DataFrame(model_input)\n",
    "        \n",
    "        # 'text'カラムの存在を確認\n",
    "        if \"text\" not in model_input.columns:\n",
    "            raise ValueError(\"Input must contain column 'text'.\")\n",
    "        \n",
    "        rows = []\n",
    "        # 各行を処理\n",
    "        for _, row in model_input.iterrows():\n",
    "            text = str(row.get(\"text\", \"\"))\n",
    "            model = str(row.get(\"model\", self.default_model))\n",
    "            \n",
    "            # プロンプトに実際のテキストを埋め込み\n",
    "            prompt_text = _render_prompt(self.prompt, text)\n",
    "            \n",
    "            # LLMを呼び出して情報抽出\n",
    "            extracted = _call_llm_return_json(\n",
    "                client=self.client,\n",
    "                prompt_text=prompt_text,\n",
    "                model=model,\n",
    "                max_tokens=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "            )\n",
    "            rows.append(extracted)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ★Models from Codeの重要なポイント★\n",
    "# set_model()を呼び出して、このファイル全体をMLflowモデルとして認識させます\n",
    "app = DocumentExtractionModel()\n",
    "set_model(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ5: モデルの記録と登録\n",
    "\n",
    "### このステップで行うこと\n",
    "1. **入力例の作成**: モデルの入力スキーマを自動推論するためのサンプルデータ\n",
    "2. **MLflow実行の開始**: 実験の記録を開始\n",
    "3. **モデルのログ記録**: モデルコード、依存関係、設定をMLflowに保存\n",
    "4. **モデルレジストリへの登録**: 本番環境へのデプロイ準備\n",
    "\n",
    "### 重要な設定項目\n",
    "- `python_model`: モデルコードのファイルパス\n",
    "- `pip_requirements`: 依存パッケージ（デプロイ時に自動インストール）\n",
    "- `model_config`: モデルに渡す設定（load_context()で参照）\n",
    "- `registered_model_name`: モデルレジストリでの名前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# 入力例の作成\n",
    "# これはモデルの入力スキーマ推論とテストに使用されます\n",
    "input_example = pd.DataFrame({\n",
    "    \"text\": [\"契約者は株式会社サンプルで、契約期間は2025年1月1日から同じ年の末までです。サービスプランはプレミアムをご契約いただいたので、月額120,000円になります。\"],\n",
    "    \"model\": [\"gpt-3.5-turbo\"],\n",
    "})\n",
    "\n",
    "# MLflow実行を開始（実験の1回の実行を表す）\n",
    "with mlflow.start_run(run_name=\"doc-extraction-model-from-code\"):\n",
    "    # PyFuncモデルとして記録\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",  # モデルの保存先（実行内のパス）\n",
    "        python_model=\"./document_extraction_model.py\",  # モデルコードのファイル\n",
    "        registered_model_name=\"document-extraction-model\",  # レジストリでの名前\n",
    "        input_example=input_example,  # 入力スキーマ推論用\n",
    "        pip_requirements=[  # 依存パッケージリスト\n",
    "            \"mlflow[genai]>=2.9.0\",\n",
    "            \"openai\",\n",
    "            \"pandas\",\n",
    "        ],\n",
    "        model_config={  # モデルに渡す設定（context.model_configで参照可能）\n",
    "            \"prompt_uri\": \"prompts:/document-extraction-system/1\",\n",
    "            \"default_model\": \"gpt-3.5-turbo\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.0,  # 決定的な出力のため0に設定\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"Model URI:\", model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ6: モデルのロードとテスト\n",
    "\n",
    "記録したモデルを実際にロードし、推論が正しく動作するかテストします。\n",
    "\n",
    "### model_uriの形式\n",
    "- `runs:/<run_id>/model`: 特定の実行からロード\n",
    "- `models:/<model_name>/<version>`: モデルレジストリからロード\n",
    "- `models:/<model_name>@<alias>`: エイリアス経由でロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをロード\n",
    "loaded = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# テスト推論を実行\n",
    "result = loaded.predict(input_example)\n",
    "print(\"推論結果:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ7: モデルエイリアスの設定\n",
    "\n",
    "### モデルエイリアスとは？\n",
    "モデルバージョンに人間が理解しやすい名前を付ける機能です。\n",
    "\n",
    "### 一般的なエイリアス名\n",
    "- `champion`: 本番環境で使用中のベストモデル\n",
    "- `challenger`: 評価中の候補モデル\n",
    "- `staging`: ステージング環境用\n",
    "\n",
    "### メリット\n",
    "- バージョン番号を直接指定せずにモデルを参照できる\n",
    "- モデル切り替えがエイリアスの付け替えだけで完結\n",
    "- A/Bテストやカナリアリリースが容易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# バージョン1に\"champion\"エイリアスを付与\n",
    "# これにより models:/document-extraction-model@champion でアクセス可能\n",
    "client.set_registered_model_alias(\n",
    "    name=\"document-extraction-model\",\n",
    "    alias=\"champion\",\n",
    "    version=\"1\"\n",
    ")\n",
    "\n",
    "print(\"エイリアス 'champion' をバージョン1に設定しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ8: モデルのサービング（デプロイ）\n",
    "\n",
    "### MLflow Models Serve\n",
    "モデルをREST APIエンドポイントとして公開する機能です。\n",
    "\n",
    "### コマンドの説明\n",
    "- `-m models:/document-extraction-model@champion`: エイリアス経由でモデルを指定\n",
    "- `-p 5000`: ポート番号\n",
    "- `--host 0.0.0.0`: すべてのネットワークインターフェースでリッスン\n",
    "\n",
    "### 使用方法\n",
    "サービング開始後、以下のようにAPIを呼び出せます:\n",
    "```bash\n",
    "curl -X POST http://localhost:5000/invocations \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"dataframe_split\": {\n",
    "      \"columns\": [\"text\", \"model\"],\n",
    "      \"data\": [[\"契約書のテキスト...\", \"gpt-3.5-turbo\"]]\n",
    "    }\n",
    "  }'\n",
    "```\n",
    "\n",
    "**注意**: 本番環境では、Databricks Model ServingやKubernetes等を使用することを推奨します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mlflow models serve \\\n",
    "  -m models:/document-extraction-model@champion \\\n",
    "  -p 5000 \\\n",
    "  --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **Prompt Registry**: プロンプトのバージョン管理と再利用\n",
    "2. **Models from Code**: コードベースでのモデル定義と登録\n",
    "3. **カスタムPyFuncモデル**: 柔軟なモデル実装パターン\n",
    "4. **MLflowトレーシング**: LLM呼び出しの可観測性向上\n",
    "5. **モデルエイリアス**: バージョン管理のベストプラクティス\n",
    "6. **モデルサービング**: REST APIとしてのデプロイ\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- 異なるLLMモデル（GPT-4、Claude等）でのテスト\n",
    "- プロンプトの改良とバージョン比較\n",
    "- 本番環境へのデプロイ\n",
    "- モニタリングとA/Bテストの実装\n",
    "\n",
    "### 参考リソース\n",
    "\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLflow Prompt Engineering](https://mlflow.org/docs/latest/llms/prompt-engineering/index.html)\n",
    "- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)"
   ]
  }
 ]
}