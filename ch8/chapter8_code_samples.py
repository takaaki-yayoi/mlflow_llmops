# ç¬¬8ç«  ç›£è¦–ã¨é‹ç”¨ - ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«é›†
# MLflowã§å®Ÿè·µã™ã‚‹LLMOps

"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€ç¬¬8ç« ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
æœ¬æ–‡ã§ã¯æŠœç²‹ã®ã¿ã‚’æ²è¼‰ã—ã€å®Œå…¨ãªå®Ÿè£…ã¯ã“ã¡ã‚‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
"""

# =============================================================================
# 8.1 æœ¬ç•ªç’°å¢ƒã§ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ç›£è¦–
# =============================================================================

# -----------------------------------------------------------------------------
# 8.1.3 æœ¬ç•ªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®åŸºæœ¬è¨­å®š
# -----------------------------------------------------------------------------

import mlflow
import os
import atexit
import signal
from datetime import datetime

def setup_production_tracing(
    service_name: str,
    environment: str = "production",
    tracking_uri: str = "databricks"
):
    """æœ¬ç•ªç’°å¢ƒå‘ã‘ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°è¨­å®š"""
    
    # éåŒæœŸãƒ­ã‚°è¨˜éŒ²ã‚’æœ‰åŠ¹åŒ–ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯å¿…é ˆï¼‰
    os.environ["MLFLOW_ENABLE_ASYNC_TRACE_LOGGING"] = "true"
    
    # ã‚µãƒ¼ãƒ“ã‚¹åã®è¨­å®šï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã«ä½¿ç”¨ï¼‰
    os.environ["OTEL_SERVICE_NAME"] = service_name
    
    # ç’°å¢ƒã‚¿ã‚°ã®è¨­å®š
    os.environ["MLFLOW_TRACE_ENVIRONMENT"] = environment
    
    # MLflowæ¥ç¶šè¨­å®š
    mlflow.set_tracking_uri(tracking_uri)
    
    # æœ¬ç•ªç”¨Experimentã®ä½œæˆã¾ãŸã¯å–å¾—
    experiment_name = f"/{environment}/{service_name}/{datetime.now().strftime('%Y-%m')}"
    mlflow.set_experiment(experiment_name)
    
    # ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã®è¨­å®š
    def graceful_shutdown(signum=None, frame=None):
        print("Flushing pending traces...")
        mlflow.flush_trace_async_logging()
        print("Trace flushing complete.")
    
    signal.signal(signal.SIGTERM, graceful_shutdown)
    signal.signal(signal.SIGINT, graceful_shutdown)
    atexit.register(graceful_shutdown)
    
    # è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®æœ‰åŠ¹åŒ–
    mlflow.openai.autolog()
    
    print(f"MLflow Tracing initialized for experiment: {experiment_name}")


# -----------------------------------------------------------------------------
# 8.1.5 ãƒˆãƒ¬ãƒ¼ã‚¹ã¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
# -----------------------------------------------------------------------------

from typing import Optional
import uuid

class TracingContext:
    """ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        user_id: str,
        session_id: str,
        request_id: Optional[str] = None,
        environment: str = "production"
    ):
        self.user_id = user_id
        self.session_id = session_id
        self.request_id = request_id or str(uuid.uuid4())
        self.environment = environment
    
    def apply_to_trace(self):
        """ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’é©ç”¨"""
        mlflow.update_current_trace(tags={
            # æ¨™æº–ã‚¿ã‚°
            "mlflow.trace.user": self.user_id,
            "mlflow.trace.session": self.session_id,
            "mlflow.trace.request_id": self.request_id,
            
            # ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°
            "environment": self.environment,
            "service.version": os.getenv("SERVICE_VERSION", "unknown"),
            "deployment.region": os.getenv("DEPLOYMENT_REGION", "unknown"),
        })


# -----------------------------------------------------------------------------
# 8.1.6 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥
# -----------------------------------------------------------------------------

import random
import time
import threading
from functools import wraps
from collections import deque
from datetime import timedelta
from typing import Callable, Set

# ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
def probabilistic_trace(sample_rate: float = 0.1):
    """ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    
    Args:
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.0-1.0ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            should_trace = random.random() < sample_rate
            
            if should_trace:
                with mlflow.start_span(name=func.__name__) as span:
                    span.set_attributes({"sampling.rate": sample_rate})
                    result = func(*args, **kwargs)
                    return result
            else:
                return func(*args, **kwargs)
        
        return wrapper
    return decorator


# æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
class ConditionalSampler:
    """æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        base_sample_rate: float = 0.1,
        error_sample_rate: float = 1.0,
        slow_request_threshold_ms: float = 5000,
        priority_users: Optional[Set[str]] = None
    ):
        self.base_sample_rate = base_sample_rate
        self.error_sample_rate = error_sample_rate
        self.slow_request_threshold_ms = slow_request_threshold_ms
        self.priority_users = priority_users or set()
    
    def should_trace(
        self,
        user_id: Optional[str] = None,
        is_error: bool = False,
        latency_ms: Optional[float] = None
    ) -> bool:
        """ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        
        # ã‚¨ãƒ©ãƒ¼ã¯å¸¸ã«ãƒˆãƒ¬ãƒ¼ã‚¹
        if is_error and random.random() < self.error_sample_rate:
            return True
        
        # å„ªå…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å¸¸ã«ãƒˆãƒ¬ãƒ¼ã‚¹
        if user_id and user_id in self.priority_users:
            return True
        
        # é…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯å¸¸ã«ãƒˆãƒ¬ãƒ¼ã‚¹
        if latency_ms and latency_ms > self.slow_request_threshold_ms:
            return True
        
        # ãã‚Œä»¥å¤–ã¯ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return random.random() < self.base_sample_rate


# é©å¿œå‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
class AdaptiveSampler:
    """è² è·ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ã‚’èª¿æ•´ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ©ãƒ¼"""
    
    def __init__(
        self,
        target_traces_per_minute: int = 100,
        min_sample_rate: float = 0.01,
        max_sample_rate: float = 1.0,
        adjustment_interval_seconds: int = 60
    ):
        self.target_traces_per_minute = target_traces_per_minute
        self.min_sample_rate = min_sample_rate
        self.max_sample_rate = max_sample_rate
        self.adjustment_interval = adjustment_interval_seconds
        
        self.current_sample_rate = max_sample_rate
        self.request_counts = deque(maxlen=60)
        self.trace_counts = deque(maxlen=60)
        
        self._lock = threading.Lock()
        self._start_adjustment_thread()
    
    def _start_adjustment_thread(self):
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡èª¿æ•´ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        def adjust_loop():
            while True:
                time.sleep(self.adjustment_interval)
                self._adjust_sample_rate()
        
        thread = threading.Thread(target=adjust_loop, daemon=True)
        thread.start()
    
    def _adjust_sample_rate(self):
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ã‚’èª¿æ•´"""
        with self._lock:
            if len(self.trace_counts) == 0:
                return
            
            current_traces_per_minute = sum(self.trace_counts)
            current_requests_per_minute = sum(self.request_counts)
            
            if current_requests_per_minute == 0:
                return
            
            ideal_rate = self.target_traces_per_minute / current_requests_per_minute
            new_rate = (self.current_sample_rate + ideal_rate) / 2
            
            self.current_sample_rate = max(
                self.min_sample_rate,
                min(self.max_sample_rate, new_rate)
            )
    
    def record_request(self, was_traced: bool):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨˜éŒ²"""
        with self._lock:
            current_second = int(time.time()) % 60
            
            if len(self.request_counts) <= current_second:
                self.request_counts.append(0)
                self.trace_counts.append(0)
            
            self.request_counts[-1] += 1
            if was_traced:
                self.trace_counts[-1] += 1
    
    def should_trace(self) -> bool:
        """ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return random.random() < self.current_sample_rate


# =============================================================================
# 8.2 ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã®å¯è¦–åŒ–
# =============================================================================

# -----------------------------------------------------------------------------
# 8.2.3 ã‚³ã‚¹ãƒˆè¨ˆç®—ã®å®Ÿè£…
# -----------------------------------------------------------------------------

from dataclasses import dataclass
from typing import Dict, List
import json

@dataclass
class ModelPricing:
    """ãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘æƒ…å ±"""
    model_name: str
    input_price_per_1k: float  # USD per 1K tokens
    output_price_per_1k: float
    cached_input_price_per_1k: Optional[float] = None
    effective_date: Optional[datetime] = None
    
    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int,
        cached_input_tokens: int = 0
    ) -> Dict[str, float]:
        """ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
        input_cost = (input_tokens / 1000) * self.input_price_per_1k
        output_cost = (output_tokens / 1000) * self.output_price_per_1k
        
        cached_cost = 0
        if cached_input_tokens and self.cached_input_price_per_1k:
            cached_cost = (cached_input_tokens / 1000) * self.cached_input_price_per_1k
        
        return {
            "input_cost": input_cost,
            "output_cost": output_cost,
            "cached_input_cost": cached_cost,
            "total_cost": input_cost + output_cost + cached_cost
        }


class CostCalculator:
    """LLMã‚³ã‚¹ãƒˆè¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, pricing_config_path: Optional[str] = None):
        self.pricing_models: Dict[str, ModelPricing] = {}
        self._load_default_pricing()
        
        if pricing_config_path:
            self._load_pricing_config(pricing_config_path)
    
    def _load_default_pricing(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ–™é‡‘è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆ2024å¹´12æœˆæ™‚ç‚¹ï¼‰"""
        default_pricing = [
            # OpenAI Models
            ModelPricing("gpt-4o", 0.0025, 0.01, 0.00125),
            ModelPricing("gpt-4o-mini", 0.00015, 0.0006, 0.000075),
            ModelPricing("gpt-4-turbo", 0.01, 0.03),
            ModelPricing("gpt-3.5-turbo", 0.0005, 0.0015),
            ModelPricing("o1-preview", 0.015, 0.06),
            ModelPricing("o1-mini", 0.003, 0.012),
            
            # Anthropic Models
            ModelPricing("claude-3-5-sonnet-20241022", 0.003, 0.015, 0.0003),
            ModelPricing("claude-3-5-haiku-20241022", 0.0008, 0.004, 0.00008),
            ModelPricing("claude-3-opus-20240229", 0.015, 0.075, 0.0015),
            
            # Google Models
            ModelPricing("gemini-1.5-pro", 0.00125, 0.005),
            ModelPricing("gemini-1.5-flash", 0.000075, 0.0003),
        ]
        
        for pricing in default_pricing:
            self.pricing_models[pricing.model_name] = pricing
    
    def _load_pricing_config(self, config_path: str):
        """å¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–™é‡‘ã‚’ãƒ­ãƒ¼ãƒ‰"""
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        for model_config in config.get("models", []):
            pricing = ModelPricing(
                model_name=model_config["name"],
                input_price_per_1k=model_config["input_price_per_1k"],
                output_price_per_1k=model_config["output_price_per_1k"],
                cached_input_price_per_1k=model_config.get("cached_input_price_per_1k"),
                effective_date=datetime.fromisoformat(model_config["effective_date"])
                    if model_config.get("effective_date") else None
            )
            self.pricing_models[pricing.model_name] = pricing
    
    def calculate_cost(
        self,
        model_name: str,
        input_tokens: int,
        output_tokens: int,
        cached_input_tokens: int = 0
    ) -> Dict[str, float]:
        """æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
        normalized_name = self._normalize_model_name(model_name)
        
        if normalized_name not in self.pricing_models:
            raise ValueError(f"Unknown model: {model_name}")
        
        pricing = self.pricing_models[normalized_name]
        return pricing.calculate_cost(input_tokens, output_tokens, cached_input_tokens)
    
    def _normalize_model_name(self, model_name: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«åã‚’æ­£è¦åŒ–"""
        aliases = {
            "gpt-4o-2024-11-20": "gpt-4o",
            "claude-3-5-sonnet-latest": "claude-3-5-sonnet-20241022",
        }
        return aliases.get(model_name, model_name)
    
    def get_available_models(self) -> list:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        return list(self.pricing_models.keys())


# ã‚³ã‚¹ãƒˆè¿½è·¡ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢
class CostTrackingMiddleware:
    """ã‚³ã‚¹ãƒˆè¿½è·¡ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢"""
    
    def __init__(self, calculator: CostCalculator):
        self.calculator = calculator
    
    def track_cost(self, model_name: str):
        """ã‚³ã‚¹ãƒˆè¿½è·¡ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                result = func(*args, **kwargs)
                
                trace_id = mlflow.get_last_active_trace_id()
                if not trace_id:
                    return result
                
                trace = mlflow.get_trace(trace_id=trace_id)
                token_usage = trace.info.token_usage
                
                if token_usage:
                    cost = self.calculator.calculate_cost(
                        model_name=model_name,
                        input_tokens=token_usage.get("input_tokens", 0),
                        output_tokens=token_usage.get("output_tokens", 0)
                    )
                    
                    mlflow.update_current_trace(tags={
                        "cost.input_usd": str(cost["input_cost"]),
                        "cost.output_usd": str(cost["output_cost"]),
                        "cost.total_usd": str(cost["total_cost"]),
                        "cost.model": model_name
                    })
                
                return result
            return wrapper
        return decorator


# -----------------------------------------------------------------------------
# 8.2.4 ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# -----------------------------------------------------------------------------

import pandas as pd

@dataclass
class CostReport:
    """ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ"""
    period_start: datetime
    period_end: datetime
    total_cost: float
    total_requests: int
    model_breakdown: Dict[str, float]
    daily_costs: List[Dict]
    top_users: List[Dict]
    anomalies: List[Dict]


class CostReporter:
    """ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
    
    def generate_weekly_report(self) -> CostReport:
        """é€±æ¬¡ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        
        traces = mlflow.search_traces(
            experiment_names=[self.experiment_name],
            filter_string=f"timestamp >= {int(start_date.timestamp() * 1000)}",
            max_results=10000
        )
        
        df = self._traces_to_dataframe(traces)
        
        if df.empty:
            return self._empty_report(start_date, end_date)
        
        total_cost = df['cost_total'].sum()
        total_requests = len(df)
        model_breakdown = df.groupby('model')['cost_total'].sum().to_dict()
        
        df['date'] = df['timestamp'].dt.date
        daily_costs = df.groupby('date').agg({
            'cost_total': 'sum',
            'request_id': 'count'
        }).reset_index().to_dict('records')
        
        top_users = df.groupby('user_id')['cost_total'].sum().nlargest(10).reset_index().to_dict('records')
        anomalies = self._detect_anomalies(df)
        
        return CostReport(
            period_start=start_date,
            period_end=end_date,
            total_cost=total_cost,
            total_requests=total_requests,
            model_breakdown=model_breakdown,
            daily_costs=daily_costs,
            top_users=top_users,
            anomalies=anomalies
        )
    
    def _traces_to_dataframe(self, traces) -> pd.DataFrame:
        """ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’DataFrameã«å¤‰æ›"""
        records = []
        for trace in traces:
            tags = trace.info.tags or {}
            records.append({
                'request_id': trace.info.request_id,
                'timestamp': pd.to_datetime(trace.info.timestamp_ms, unit='ms'),
                'user_id': tags.get('mlflow.trace.user'),
                'model': tags.get('cost.model'),
                'cost_total': float(tags.get('cost.total_usd', 0)),
                'input_tokens': trace.info.token_usage.get('input_tokens', 0) if trace.info.token_usage else 0,
                'output_tokens': trace.info.token_usage.get('output_tokens', 0) if trace.info.token_usage else 0,
            })
        return pd.DataFrame(records)
    
    def _detect_anomalies(self, df: pd.DataFrame) -> List[Dict]:
        """ã‚³ã‚¹ãƒˆç•°å¸¸ã‚’æ¤œå‡º"""
        anomalies = []
        
        mean_cost = df['cost_total'].mean()
        std_cost = df['cost_total'].std()
        threshold = mean_cost + 3 * std_cost
        
        high_cost_requests = df[df['cost_total'] > threshold]
        for _, row in high_cost_requests.iterrows():
            anomalies.append({
                'type': 'high_cost_request',
                'request_id': row['request_id'],
                'cost': row['cost_total'],
                'threshold': threshold,
                'timestamp': row['timestamp'].isoformat()
            })
        
        return anomalies
    
    def _empty_report(self, start_date: datetime, end_date: datetime) -> CostReport:
        return CostReport(
            period_start=start_date,
            period_end=end_date,
            total_cost=0,
            total_requests=0,
            model_breakdown={},
            daily_costs=[],
            top_users=[],
            anomalies=[]
        )


# =============================================================================
# 8.3 å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¿½è·¡
# =============================================================================

# -----------------------------------------------------------------------------
# 8.3.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
# -----------------------------------------------------------------------------

import statistics

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float
    throughput_rps: float
    error_rate: float
    timeout_rate: float


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, window_size_seconds: int = 300):
        self.window_size = timedelta(seconds=window_size_seconds)
        self.metrics_buffer: List[Dict] = []
    
    def record_request(
        self,
        latency_ms: float,
        is_error: bool = False,
        is_timeout: bool = False
    ):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        self.metrics_buffer.append({
            'timestamp': datetime.now(),
            'latency_ms': latency_ms,
            'is_error': is_error,
            'is_timeout': is_timeout
        })
        
        cutoff = datetime.now() - self.window_size
        self.metrics_buffer = [
            m for m in self.metrics_buffer 
            if m['timestamp'] > cutoff
        ]
    
    def get_current_metrics(self) -> PerformanceMetrics:
        """ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        if not self.metrics_buffer:
            return PerformanceMetrics(0, 0, 0, 0, 0, 0)
        
        latencies = [m['latency_ms'] for m in self.metrics_buffer]
        latencies_sorted = sorted(latencies)
        n = len(latencies)
        
        return PerformanceMetrics(
            latency_p50_ms=latencies_sorted[int(n * 0.50)],
            latency_p95_ms=latencies_sorted[int(n * 0.95)],
            latency_p99_ms=latencies_sorted[int(n * 0.99)] if n > 100 else latencies_sorted[-1],
            throughput_rps=n / self.window_size.total_seconds(),
            error_rate=sum(1 for m in self.metrics_buffer if m['is_error']) / n,
            timeout_rate=sum(1 for m in self.metrics_buffer if m['is_timeout']) / n
        )


# -----------------------------------------------------------------------------
# 8.3.3 ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
# -----------------------------------------------------------------------------

from enum import Enum
from typing import Any

class FeedbackType(Enum):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç¨®é¡"""
    THUMBS_UP = "thumbs_up"
    THUMBS_DOWN = "thumbs_down"
    RATING = "rating"
    TEXT_COMMENT = "text_comment"
    CORRECTION = "correction"
    REPORT = "report"


@dataclass
class UserFeedback:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯"""
    feedback_id: str
    trace_id: str
    user_id: str
    feedback_type: FeedbackType
    value: Any
    comment: Optional[str] = None
    timestamp: datetime = None
    metadata: Optional[Dict] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.feedback_id is None:
            self.feedback_id = str(uuid.uuid4())


class FeedbackCollector:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.feedback_buffer: List[UserFeedback] = []
    
    def record_thumbs_feedback(
        self,
        trace_id: str,
        user_id: str,
        is_positive: bool,
        comment: Optional[str] = None
    ) -> UserFeedback:
        """ã‚µãƒ ã‚ºã‚¢ãƒƒãƒ—/ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²"""
        feedback_type = FeedbackType.THUMBS_UP if is_positive else FeedbackType.THUMBS_DOWN
        
        feedback = UserFeedback(
            feedback_id=str(uuid.uuid4()),
            trace_id=trace_id,
            user_id=user_id,
            feedback_type=feedback_type,
            value=is_positive,
            comment=comment
        )
        
        self._store_feedback(feedback)
        return feedback
    
    def record_rating(
        self,
        trace_id: str,
        user_id: str,
        rating: int,
        comment: Optional[str] = None
    ) -> UserFeedback:
        """è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆ1-5ï¼‰ã‚’è¨˜éŒ²"""
        if not 1 <= rating <= 5:
            raise ValueError("Rating must be between 1 and 5")
        
        feedback = UserFeedback(
            feedback_id=str(uuid.uuid4()),
            trace_id=trace_id,
            user_id=user_id,
            feedback_type=FeedbackType.RATING,
            value=rating,
            comment=comment
        )
        
        self._store_feedback(feedback)
        return feedback
    
    def _store_feedback(self, feedback: UserFeedback):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜"""
        self.feedback_buffer.append(feedback)
        
        try:
            mlflow.log_feedback(
                trace_id=feedback.trace_id,
                name=feedback.feedback_type.value,
                value=feedback.value,
                comment=feedback.comment
            )
        except Exception as e:
            print(f"Failed to log feedback to MLflow: {e}")
    
    def get_feedback_stats(self, hours: int = 24) -> Dict:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±è¨ˆã‚’å–å¾—"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent = [f for f in self.feedback_buffer if f.timestamp > cutoff]
        
        thumbs_up = sum(1 for f in recent if f.feedback_type == FeedbackType.THUMBS_UP)
        thumbs_down = sum(1 for f in recent if f.feedback_type == FeedbackType.THUMBS_DOWN)
        ratings = [f.value for f in recent if f.feedback_type == FeedbackType.RATING]
        
        return {
            'total_feedback': len(recent),
            'thumbs_up': thumbs_up,
            'thumbs_down': thumbs_down,
            'positive_rate': thumbs_up / (thumbs_up + thumbs_down) if (thumbs_up + thumbs_down) > 0 else None,
            'average_rating': statistics.mean(ratings) if ratings else None,
        }


# -----------------------------------------------------------------------------
# 8.3.5 ç¶™ç¶šçš„è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------------------------

import schedule
from mlflow.genai import evaluate, create_dataset
from mlflow.genai.scorers import RetrievalGroundedness, RelevanceToQuery, Safety


class ContinuousEvaluationPipeline:
    """ç¶™ç¶šçš„è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        experiment_name: str,
        scorers: List,
        sample_rate: float = 0.1,
        evaluation_interval_minutes: int = 60,
        alert_thresholds: Optional[Dict] = None
    ):
        self.experiment_name = experiment_name
        self.scorers = scorers
        self.sample_rate = sample_rate
        self.evaluation_interval = evaluation_interval_minutes
        self.alert_thresholds = alert_thresholds or {
            'relevance': 0.7,
            'safety': 0.9,
            'groundedness': 0.7
        }
        
        self.evaluation_history: List[Dict] = []
        self.alert_handlers: List[Callable] = []
    
    def add_alert_handler(self, handler: Callable):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ """
        self.alert_handlers.append(handler)
    
    def run_evaluation(self) -> Dict:
        """è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        print(f"[{datetime.now()}] Starting evaluation...")
        
        since = datetime.now() - timedelta(minutes=self.evaluation_interval)
        traces = mlflow.search_traces(
            experiment_names=[self.experiment_name],
            filter_string=f"timestamp >= {int(since.timestamp() * 1000)}",
            max_results=1000
        )
        
        if not traces:
            print("No traces found for evaluation")
            return {}
        
        sample_size = max(1, int(len(traces) * self.sample_rate))
        sampled_traces = random.sample(list(traces), sample_size)
        
        dataset = create_dataset(f"continuous-eval-{datetime.now().strftime('%Y%m%d%H%M')}")
        dataset.insert(sampled_traces)
        
        results = evaluate(data=dataset, scorers=self.scorers)
        
        evaluation_result = {
            'timestamp': datetime.now().isoformat(),
            'traces_evaluated': sample_size,
            'total_traces': len(traces),
            'metrics': results.metrics
        }
        self.evaluation_history.append(evaluation_result)
        
        self._check_thresholds(results.metrics)
        
        print(f"Evaluation complete. Metrics: {results.metrics}")
        return evaluation_result
    
    def _check_thresholds(self, metrics: Dict):
        """é–¾å€¤ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç™ºç«"""
        for metric_name, threshold in self.alert_thresholds.items():
            if metric_name in metrics and metrics[metric_name] < threshold:
                alert = {
                    'type': 'quality_threshold_violation',
                    'metric': metric_name,
                    'value': metrics[metric_name],
                    'threshold': threshold,
                    'timestamp': datetime.now().isoformat()
                }
                
                for handler in self.alert_handlers:
                    try:
                        handler(alert)
                    except Exception as e:
                        print(f"Alert handler failed: {e}")
    
    def start_scheduler(self):
        """å®šæœŸå®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’é–‹å§‹"""
        schedule.every(self.evaluation_interval).minutes.do(self.run_evaluation)
        
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)
        
        thread = threading.Thread(target=run_scheduler, daemon=True)
        thread.start()
        print(f"Scheduler started. Running every {self.evaluation_interval} minutes.")


# =============================================================================
# 8.4 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¨ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
# =============================================================================

# -----------------------------------------------------------------------------
# 8.4.1 ã‚¢ãƒ©ãƒ¼ãƒˆæˆ¦ç•¥ã®è¨­è¨ˆ
# -----------------------------------------------------------------------------

class AlertSeverity(Enum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆã®é‡è¦åº¦"""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"


class AlertCategory(Enum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆã®ã‚«ãƒ†ã‚´ãƒª"""
    AVAILABILITY = "availability"
    PERFORMANCE = "performance"
    QUALITY = "quality"
    COST = "cost"
    SECURITY = "security"


@dataclass
class Alert:
    """ã‚¢ãƒ©ãƒ¼ãƒˆ"""
    alert_id: str
    category: AlertCategory
    severity: AlertSeverity
    title: str
    description: str
    metric_name: str
    current_value: float
    threshold: float
    timestamp: datetime
    metadata: Optional[Dict] = None
    acknowledged: bool = False
    resolved: bool = False
    
    def to_dict(self) -> Dict:
        return {
            'alert_id': self.alert_id,
            'category': self.category.value,
            'severity': self.severity.value,
            'title': self.title,
            'description': self.description,
            'metric_name': self.metric_name,
            'current_value': self.current_value,
            'threshold': self.threshold,
            'timestamp': self.timestamp.isoformat(),
            'metadata': self.metadata
        }


class AlertRule:
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«"""
    
    def __init__(
        self,
        name: str,
        category: AlertCategory,
        metric_name: str,
        condition: Callable[[float], bool],
        threshold: float,
        severity: AlertSeverity,
        title_template: str,
        description_template: str,
        cooldown_minutes: int = 15
    ):
        self.name = name
        self.category = category
        self.metric_name = metric_name
        self.condition = condition
        self.threshold = threshold
        self.severity = severity
        self.title_template = title_template
        self.description_template = description_template
        self.cooldown = timedelta(minutes=cooldown_minutes)
        self.last_fired: Optional[datetime] = None
    
    def evaluate(self, current_value: float) -> Optional[Alert]:
        """ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡ã—ã¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.condition(current_value):
            return None
        
        if self.last_fired and (datetime.now() - self.last_fired) < self.cooldown:
            return None
        
        self.last_fired = datetime.now()
        
        return Alert(
            alert_id=f"{self.name}-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            category=self.category,
            severity=self.severity,
            title=self.title_template.format(value=current_value, threshold=self.threshold),
            description=self.description_template.format(value=current_value, threshold=self.threshold),
            metric_name=self.metric_name,
            current_value=current_value,
            threshold=self.threshold,
            timestamp=datetime.now()
        )


# -----------------------------------------------------------------------------
# 8.4.3 ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
# -----------------------------------------------------------------------------

from abc import ABC, abstractmethod
import requests

class AlertNotifier(ABC):
    """ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def send(self, alert: Alert) -> bool:
        pass


class SlackNotifier(AlertNotifier):
    """Slacké€šçŸ¥"""
    
    def __init__(self, webhook_url: str, channel: Optional[str] = None):
        self.webhook_url = webhook_url
        self.channel = channel
    
    def send(self, alert: Alert) -> bool:
        severity_emoji = {
            AlertSeverity.CRITICAL: "ğŸš¨",
            AlertSeverity.WARNING: "âš ï¸",
            AlertSeverity.INFO: "â„¹ï¸"
        }
        
        severity_color = {
            AlertSeverity.CRITICAL: "#FF0000",
            AlertSeverity.WARNING: "#FFA500",
            AlertSeverity.INFO: "#0000FF"
        }
        
        payload = {
            "channel": self.channel,
            "attachments": [{
                "color": severity_color[alert.severity],
                "blocks": [
                    {
                        "type": "header",
                        "text": {
                            "type": "plain_text",
                            "text": f"{severity_emoji[alert.severity]} {alert.title}"
                        }
                    },
                    {
                        "type": "section",
                        "fields": [
                            {"type": "mrkdwn", "text": f"*ã‚«ãƒ†ã‚´ãƒª:*\n{alert.category.value}"},
                            {"type": "mrkdwn", "text": f"*é‡è¦åº¦:*\n{alert.severity.value}"},
                            {"type": "mrkdwn", "text": f"*ãƒ¡ãƒˆãƒªã‚¯ã‚¹:*\n{alert.metric_name}"},
                            {"type": "mrkdwn", "text": f"*ç¾åœ¨å€¤:*\n{alert.current_value:.4f}"},
                        ]
                    },
                    {
                        "type": "section",
                        "text": {"type": "mrkdwn", "text": alert.description}
                    },
                ]
            }]
        }
        
        try:
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            return response.status_code == 200
        except Exception as e:
            print(f"Slack notification failed: {e}")
            return False


class AlertManager:
    """ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.rules: List[AlertRule] = []
        self.notifiers: Dict[AlertSeverity, List[AlertNotifier]] = {
            AlertSeverity.CRITICAL: [],
            AlertSeverity.WARNING: [],
            AlertSeverity.INFO: []
        }
        self.alert_history: List[Alert] = []
    
    def add_rule(self, rule: AlertRule):
        """ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ """
        self.rules.append(rule)
    
    def add_notifier(
        self,
        notifier: AlertNotifier,
        severities: List[AlertSeverity] = None
    ):
        """é€šçŸ¥å…ˆã‚’è¿½åŠ """
        if severities is None:
            severities = list(AlertSeverity)
        
        for severity in severities:
            self.notifiers[severity].append(notifier)
    
    def evaluate_metrics(self, metrics: Dict[str, float]) -> List[Alert]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è©•ä¾¡ã—ã¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        alerts = []
        
        for rule in self.rules:
            if rule.metric_name in metrics:
                alert = rule.evaluate(metrics[rule.metric_name])
                if alert:
                    alerts.append(alert)
                    self.alert_history.append(alert)
                    self._send_notifications(alert)
        
        return alerts
    
    def _send_notifications(self, alert: Alert):
        """é€šçŸ¥ã‚’é€ä¿¡"""
        notifiers = self.notifiers.get(alert.severity, [])
        
        for notifier in notifiers:
            try:
                success = notifier.send(alert)
                if not success:
                    print(f"Notification failed for {type(notifier).__name__}")
            except Exception as e:
                print(f"Notification error: {e}")


# =============================================================================
# 8.5 OpenTelemetryã¨ã®çµ±åˆ
# =============================================================================

# -----------------------------------------------------------------------------
# 8.5.2 OTLPè¨­å®š
# -----------------------------------------------------------------------------

class OTelConfiguration:
    """OpenTelemetryè¨­å®šã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹"""
    
    @staticmethod
    def configure_production(
        service_name: str,
        service_version: str,
        environment: str,
        collector_endpoint: str,
        sample_rate: float = 1.0,
        enable_metrics: bool = True,
        enable_logs: bool = False
    ):
        """æœ¬ç•ªç’°å¢ƒå‘ã‘OTelè¨­å®š"""
        
        os.environ["OTEL_SERVICE_NAME"] = service_name
        os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = collector_endpoint
        os.environ["OTEL_EXPORTER_OTLP_PROTOCOL"] = "grpc"
        
        resource_attrs = [
            f"service.name={service_name}",
            f"service.version={service_version}",
            f"deployment.environment={environment}",
            f"service.namespace=llm-applications",
        ]
        os.environ["OTEL_RESOURCE_ATTRIBUTES"] = ",".join(resource_attrs)
        
        if sample_rate < 1.0:
            os.environ["OTEL_TRACES_SAMPLER"] = "parentbased_traceidratio"
            os.environ["OTEL_TRACES_SAMPLER_ARG"] = str(sample_rate)
        
        if enable_metrics:
            os.environ["OTEL_METRICS_EXPORTER"] = "otlp"
        else:
            os.environ["OTEL_METRICS_EXPORTER"] = "none"
        
        if enable_logs:
            os.environ["OTEL_LOGS_EXPORTER"] = "otlp"
        else:
            os.environ["OTEL_LOGS_EXPORTER"] = "none"
        
        os.environ["OTEL_BSP_SCHEDULE_DELAY"] = "5000"
        os.environ["OTEL_BSP_MAX_EXPORT_BATCH_SIZE"] = "512"
        os.environ["OTEL_BSP_MAX_QUEUE_SIZE"] = "2048"
        os.environ["OTEL_EXPORTER_OTLP_TIMEOUT"] = "10000"
    
    @staticmethod
    def configure_development():
        """é–‹ç™ºç’°å¢ƒå‘ã‘OTelè¨­å®š"""
        os.environ["OTEL_SERVICE_NAME"] = "llm-app-dev"
        os.environ["OTEL_TRACES_EXPORTER"] = "console"
        os.environ["OTEL_METRICS_EXPORTER"] = "none"
        os.environ["OTEL_LOGS_EXPORTER"] = "none"


# -----------------------------------------------------------------------------
# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹
# -----------------------------------------------------------------------------

from prometheus_client import Counter, Histogram, Gauge, start_http_server

LLM_REQUEST_COUNT = Counter(
    'llm_requests_total',
    'Total number of LLM requests',
    ['model', 'status', 'endpoint']
)

LLM_LATENCY = Histogram(
    'llm_request_duration_seconds',
    'LLM request latency in seconds',
    ['model', 'endpoint'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0]
)

LLM_TOKENS = Counter(
    'llm_tokens_total',
    'Total tokens used',
    ['model', 'token_type']
)

LLM_COST = Counter(
    'llm_cost_usd_total',
    'Total cost in USD',
    ['model']
)


class PrometheusMetricsCollector:
    """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, port: int = 8000):
        self.port = port
        start_http_server(port)
        print(f"Prometheus metrics server started on port {port}")
    
    def record_request(
        self,
        model: str,
        endpoint: str,
        latency_seconds: float,
        input_tokens: int,
        output_tokens: int,
        cost_usd: float,
        status: str = "success"
    ):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        LLM_REQUEST_COUNT.labels(model=model, status=status, endpoint=endpoint).inc()
        LLM_LATENCY.labels(model=model, endpoint=endpoint).observe(latency_seconds)
        LLM_TOKENS.labels(model=model, token_type="input").inc(input_tokens)
        LLM_TOKENS.labels(model=model, token_type="output").inc(output_tokens)
        LLM_COST.labels(model=model).inc(cost_usd)
